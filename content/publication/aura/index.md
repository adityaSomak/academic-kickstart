+++
abstract = ""
abstract_short = ""
authors = ["Sayantan Adak", "Pratyush Chatterjee", "Somnath Banerjee", "Rima Hazra", "Somak Aditya", "Animesh Mukherjee"]
date = "2025-10-08"
image_preview = ""
math = true
publication_types = ["1"]
publication = "In AAAI 2026 AIA Track"
publication_short = "In <span style='color:brown;'>*AAAI 2026 (AIA Track)*</span>"
selected = true
featured = true
title = "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models"
projects = ["nlp","Vision"]
url_pdf = "https://arxiv.org/pdf/2508.06124"
url_code = ""
url_project = ""
url_dataset = ""
url_video = ""


# Optional featured image (relative to `static/img/` folder).
# [header]
# image = ""
# caption = "My caption :smile:"
# focal_point = "center"

[image]
caption = ""
focal_point = "center"
+++

Present day LLMs face the challenge of managing affordance-based safety risksâ€”situations where outputs inadvertently facilitate harmful actions due to overlooked logical implications. Traditional safety solutions, such as scalar outcome-based reward models, parameter tuning, or heuristic decoding strategies, lack the granularity and proactive nature needed to reliably detect and intervene during subtle yet crucial reasoning steps. Addressing this fundamental gap, we introduce AURA, an innovative, multi-layered framework centered around Process Reward Models (PRMs), providing comprehensive, step level evaluations across logical coherence and safety-awareness. Our framework seamlessly combines introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding to dynamically and proactively guide models toward safer reasoning trajectories. Empirical evidence clearly demonstrates that this approach significantly surpasses existing methods, significantly improving the logical integrity and affordance-sensitive safety of model outputs. This research represents a pivotal step toward safer, more responsible, and contextually aware AI, setting a new benchmark for alignment-sensitive applications.